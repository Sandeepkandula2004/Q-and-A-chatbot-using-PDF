# ğŸ“„ Chat with Your PDF

A Streamlit-powered RAG (Retrieval-Augmented Generation) app that lets you **upload a PDF** and **ask questions about it** using LLaMA 3 (via Groq), Chroma vector store, and HuggingFace embeddings.

---

## ğŸš€ Features

- ğŸ¤– Natural language Q&A with your uploaded PDF
- ğŸ§  LangChain history-aware retriever
- ğŸ” Chroma vector DB for semantic search
- ğŸ§© `llama3-8b-8192` via Groq API
- ğŸ’¬ Session-based chat memory

---

## ğŸ› ï¸ Requirements

- Python 3.9+
- Groq API Key â†’ [https://console.groq.com](https://console.groq.com)
- Hugging Face Token â†’ [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/Sandeepkandula2004/Q-and-A-chatbot-using-PDF
cd Q-and-A-chatbot-using-PDF

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ” Setup HuggingFace API Key

Set your Hugging Face API token as an environment variable before running:

### On Linux/macOS:
```bash
export HUGGINGFACE_KEY=your_huggingface_token
```

### On Windows (CMD):
```cmd
set HUGGINGFACE_KEY=your_huggingface_token
```

> ğŸ“Œ **NOTE:**  
> In the original code, the line:
> ```python
> os.environ["HUGGINGFACE_KEY"] = st.secrets["HUGGINGFACE_KEY"]
> ```
> should be replaced with:
> ```python
> os.environ["HUGGINGFACE_KEY"] = os.getenv("HUGGINGFACE_KEY")
> ```
> so that the token is pulled directly from your environment during local development.

---

## â–¶ï¸ Run the App

```bash
streamlit run app.py
```

Open [http://localhost:8501](http://localhost:8501) in your browser.

---

## ğŸ§  How It Works

1. PDF is parsed using `PyPDFLoader`
2. Text is split using `RecursiveCharacterTextSplitter`
3. Chunks embedded using `all-MiniLM-L6-v2` from HuggingFace
4. Embeddings stored in **ChromaDB** (in a temporary folder)
5. Questions are contextualized using LangChainâ€™s history-aware retriever
6. Answers are generated by `llama3-8b-8192` from Groq

---

## ğŸ“„ Example Questions

- "Whatâ€™s the conclusion of the paper?"
- "Explain the method section briefly."
- "List any key findings mentioned."

---

## ğŸ“ Vector Store

The vector store is saved in your temporary system directory:

```
/tmp/chroma_db  (or Windows equivalent)
```

It's re-created on each new upload.

---

## ğŸ§© Tech Stack

- LangChain
- Chroma
- HuggingFace Sentence Transformers
- Groq + LLaMA3
- Streamlit
